{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Player Detection with YOLOv8\n",
    "\n",
    "**SC549: Neural Networks - Programming Assignment 03**\n",
    "\n",
    "In this notebook, we'll:\n",
    "1. Load a pre-trained YOLOv8 model\n",
    "2. Detect players in video frames\n",
    "3. Visualize detection results\n",
    "4. Process entire videos\n",
    "5. Evaluate detection performance\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ What is YOLO?\n",
    "\n",
    "**YOLO (You Only Look Once)** is a real-time object detection system:\n",
    "- **Input**: An image\n",
    "- **Output**: Bounding boxes + class labels + confidence scores\n",
    "- **Speed**: Processes images in milliseconds (30+ FPS)\n",
    "\n",
    "### How YOLO Works (Simple Explanation)\n",
    "1. **Divide image into grid**: Split image into NxN cells\n",
    "2. **Predict boxes**: Each cell predicts bounding boxes\n",
    "3. **Classify objects**: Determine what's in each box\n",
    "4. **Filter predictions**: Keep only high-confidence detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# YOLOv8\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Check PyTorch and CUDA\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "PROJECT_ROOT = Path('../')\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "VIDEOS_DIR = DATA_DIR / 'videos'\n",
    "FRAMES_DIR = DATA_DIR / 'frames'\n",
    "OUTPUTS_DIR = PROJECT_ROOT / 'outputs'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "\n",
    "# Create output directories\n",
    "(OUTPUTS_DIR / 'videos').mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUTS_DIR / 'screenshots').mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUTS_DIR / 'metrics').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Find video files\n",
    "video_files = list(VIDEOS_DIR.glob('*.mp4')) + list(VIDEOS_DIR.glob('*.avi'))\n",
    "print(f\"ðŸ“¹ Found {len(video_files)} video(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load YOLOv8 Model\n",
    "\n",
    "### YOLOv8 Models Available:\n",
    "- **YOLOv8n** (nano): Fastest, lowest accuracy\n",
    "- **YOLOv8s** (small): Balanced\n",
    "- **YOLOv8m** (medium): Good accuracy\n",
    "- **YOLOv8l** (large): Higher accuracy\n",
    "- **YOLOv8x** (xlarge): Best accuracy, slowest\n",
    "\n",
    "For this assignment, we'll use **YOLOv8s** (small) for a good balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained YOLOv8 model\n",
    "print(\"ðŸ“¥ Loading YOLOv8 model...\")\n",
    "model = YOLO('yolov8s.pt')  # 's' = small model\n",
    "\n",
    "# Move model to device (GPU or CPU)\n",
    "model.to(device)\n",
    "\n",
    "print(f\"âœ… Model loaded on {device}\")\n",
    "print(f\"\\nModel info:\")\n",
    "print(f\"  - Architecture: YOLOv8s\")\n",
    "print(f\"  - Parameters: ~11.2M\")\n",
    "print(f\"  - Pre-trained on: COCO dataset (80 classes)\")\n",
    "print(f\"  - Person class ID: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Detection on a Single Frame\n",
    "\n",
    "Let's test the model on one frame to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_players(image, model, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Detect players (persons) in an image.\n",
    "    \n",
    "    Args:\n",
    "        image: numpy array (BGR format from OpenCV)\n",
    "        model: YOLOv8 model\n",
    "        conf_threshold: Confidence threshold for detections\n",
    "    \n",
    "    Returns:\n",
    "        results: YOLOv8 results object\n",
    "    \"\"\"\n",
    "    # Run inference\n",
    "    # classes=[0] means we only want 'person' class\n",
    "    results = model(image, classes=[0], conf=conf_threshold, verbose=False)\n",
    "    return results[0]  # Return first result (single image)\n",
    "\n",
    "# Load a test frame\n",
    "if len(video_files) > 0:\n",
    "    # Get first video\n",
    "    test_video = video_files[0]\n",
    "    cap = cv2.VideoCapture(str(test_video))\n",
    "    \n",
    "    # Read first frame\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if ret:\n",
    "        print(f\"ðŸ–¼ï¸  Testing on frame from: {test_video.name}\")\n",
    "        print(f\"   Frame shape: {frame.shape}\")\n",
    "        \n",
    "        # Detect players\n",
    "        print(\"\\nðŸ” Running detection...\")\n",
    "        results = detect_players(frame, model, conf_threshold=0.3)\n",
    "        \n",
    "        # Get detection info\n",
    "        boxes = results.boxes\n",
    "        num_detections = len(boxes)\n",
    "        \n",
    "        print(f\"âœ… Detected {num_detections} player(s)\")\n",
    "        \n",
    "        # Print details of each detection\n",
    "        for i, box in enumerate(boxes):\n",
    "            conf = box.conf[0].item()\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            print(f\"   Player {i+1}: Confidence={conf:.2f}, Box=[{x1:.0f}, {y1:.0f}, {x2:.0f}, {y2:.0f}]\")\n",
    "        \n",
    "        # Visualize\n",
    "        annotated_frame = results.plot()  # Draw boxes on image\n",
    "        \n",
    "        # Convert BGR to RGB for matplotlib\n",
    "        annotated_rgb = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Display\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(annotated_rgb)\n",
    "        plt.title(f\"Player Detection - {num_detections} players detected\", fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save\n",
    "        save_path = OUTPUTS_DIR / 'screenshots' / 'detection_test.png'\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nðŸ’¾ Saved: {save_path}\")\n",
    "    else:\n",
    "        print(\"âŒ Could not read frame\")\n",
    "else:\n",
    "    print(\"âš ï¸  No videos found. Please run notebook 01 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding Detection Results\n",
    "\n",
    "### What information does YOLO provide?\n",
    "\n",
    "For each detection:\n",
    "1. **Bounding Box**: `[x1, y1, x2, y2]` coordinates\n",
    "   - `(x1, y1)`: Top-left corner\n",
    "   - `(x2, y2)`: Bottom-right corner\n",
    "\n",
    "2. **Confidence Score**: Probability (0-1) that detection is correct\n",
    "   - 0.3 = 30% confident\n",
    "   - 0.9 = 90% confident\n",
    "\n",
    "3. **Class ID**: What object was detected\n",
    "   - 0 = Person/Player\n",
    "   - 32 = Sports ball\n",
    "   - etc.\n",
    "\n",
    "Let's explore the results object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(video_files) > 0 and ret:\n",
    "    print(\"ðŸ“Š Detection Results Structure:\\n\")\n",
    "    \n",
    "    # Bounding boxes\n",
    "    print(\"1. Bounding Boxes (xyxy format):\")\n",
    "    print(f\"   Shape: {results.boxes.xyxy.shape}\")\n",
    "    print(f\"   Data: {results.boxes.xyxy[:3]}...\")  # Show first 3\n",
    "    \n",
    "    # Confidence scores\n",
    "    print(\"\\n2. Confidence Scores:\")\n",
    "    print(f\"   Shape: {results.boxes.conf.shape}\")\n",
    "    print(f\"   Data: {results.boxes.conf}\")\n",
    "    \n",
    "    # Class IDs\n",
    "    print(\"\\n3. Class IDs:\")\n",
    "    print(f\"   Shape: {results.boxes.cls.shape}\")\n",
    "    print(f\"   Data: {results.boxes.cls}\")\n",
    "    print(\"   (All should be 0 for 'person' class)\")\n",
    "    \n",
    "    # Original image shape\n",
    "    print(\"\\n4. Original Image:\")\n",
    "    print(f\"   Shape: {results.orig_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process Complete Video\n",
    "\n",
    "Now let's process an entire video and save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_with_detection(video_path, model, output_path, conf_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Process a video and annotate with player detections.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video\n",
    "        model: YOLOv8 model\n",
    "        output_path: Path to save output video\n",
    "        conf_threshold: Detection confidence threshold\n",
    "    \n",
    "    Returns:\n",
    "        dict: Statistics (total frames, detections, etc.)\n",
    "    \"\"\"\n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"Processing: {video_path.name}\")\n",
    "    print(f\"  Resolution: {width}x{height}\")\n",
    "    print(f\"  FPS: {fps}\")\n",
    "    print(f\"  Total frames: {total_frames}\")\n",
    "    \n",
    "    # Statistics\n",
    "    stats = {\n",
    "        'total_frames': total_frames,\n",
    "        'total_detections': 0,\n",
    "        'avg_detections_per_frame': 0,\n",
    "        'frames_with_detections': 0\n",
    "    }\n",
    "    \n",
    "    # Process each frame\n",
    "    pbar = tqdm(total=total_frames, desc=\"Processing frames\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Detect players\n",
    "        results = detect_players(frame, model, conf_threshold)\n",
    "        \n",
    "        # Update statistics\n",
    "        num_detections = len(results.boxes)\n",
    "        stats['total_detections'] += num_detections\n",
    "        if num_detections > 0:\n",
    "            stats['frames_with_detections'] += 1\n",
    "        \n",
    "        # Annotate frame\n",
    "        annotated = results.plot()\n",
    "        \n",
    "        # Write frame\n",
    "        out.write(annotated)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    pbar.close()\n",
    "    \n",
    "    # Calculate averages\n",
    "    stats['avg_detections_per_frame'] = stats['total_detections'] / stats['total_frames']\n",
    "    \n",
    "    print(f\"\\nâœ… Processing complete!\")\n",
    "    print(f\"   Total detections: {stats['total_detections']}\")\n",
    "    print(f\"   Avg per frame: {stats['avg_detections_per_frame']:.2f}\")\n",
    "    print(f\"   Frames with detections: {stats['frames_with_detections']}/{stats['total_frames']}\")\n",
    "    print(f\"   Saved to: {output_path}\\n\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Process all videos\n",
    "if len(video_files) > 0:\n",
    "    print(\"ðŸŽ¬ Processing all videos with player detection...\\n\")\n",
    "    \n",
    "    all_stats = {}\n",
    "    \n",
    "    for video_path in video_files:\n",
    "        output_path = OUTPUTS_DIR / 'videos' / f\"detected_{video_path.name}\"\n",
    "        stats = process_video_with_detection(video_path, model, output_path, conf_threshold=0.3)\n",
    "        all_stats[video_path.name] = stats\n",
    "    \n",
    "    print(\"\\nâœ… All videos processed!\")\n",
    "    print(f\"ðŸ“ Output videos: {OUTPUTS_DIR / 'videos'}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No videos found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detection Performance Analysis\n",
    "\n",
    "Let's analyze the detection results across all videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(video_files) > 0:\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for video_name, stats in all_stats.items():\n",
    "        summary_data.append({\n",
    "            'Video': video_name,\n",
    "            'Total Frames': stats['total_frames'],\n",
    "            'Total Detections': stats['total_detections'],\n",
    "            'Avg Detections/Frame': f\"{stats['avg_detections_per_frame']:.2f}\",\n",
    "            'Frames with Players': stats['frames_with_detections']\n",
    "        })\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"\\nðŸ“Š Detection Summary:\\n\")\n",
    "    print(df_summary.to_string(index=False))\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Chart 1: Total detections per video\n",
    "    videos = [s['Video'][:15] for s in summary_data]  # Truncate names\n",
    "    detections = [s['Total Detections'] for s in summary_data]\n",
    "    \n",
    "    axes[0].bar(videos, detections, color='steelblue')\n",
    "    axes[0].set_xlabel('Video')\n",
    "    axes[0].set_ylabel('Total Detections')\n",
    "    axes[0].set_title('Player Detections per Video')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Chart 2: Average detections per frame\n",
    "    avg_detections = [float(s['Avg Detections/Frame']) for s in summary_data]\n",
    "    \n",
    "    axes[1].bar(videos, avg_detections, color='coral')\n",
    "    axes[1].set_xlabel('Video')\n",
    "    axes[1].set_ylabel('Avg Detections per Frame')\n",
    "    axes[1].set_title('Average Players per Frame')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    save_path = OUTPUTS_DIR / 'metrics' / 'detection_stats.png'\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Saved chart: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confidence Threshold Analysis\n",
    "\n",
    "Let's see how different confidence thresholds affect detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(video_files) > 0:\n",
    "    # Test different thresholds on first video\n",
    "    test_video = video_files[0]\n",
    "    cap = cv2.VideoCapture(str(test_video))\n",
    "    ret, test_frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if ret:\n",
    "        thresholds = [0.1, 0.25, 0.5, 0.7, 0.9]\n",
    "        detection_counts = []\n",
    "        \n",
    "        print(\"ðŸ” Testing different confidence thresholds...\\n\")\n",
    "        \n",
    "        for thresh in thresholds:\n",
    "            results = detect_players(test_frame, model, conf_threshold=thresh)\n",
    "            count = len(results.boxes)\n",
    "            detection_counts.append(count)\n",
    "            print(f\"  Threshold {thresh:.2f}: {count} detections\")\n",
    "        \n",
    "        # Visualize\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(thresholds, detection_counts, marker='o', linewidth=2, markersize=8)\n",
    "        plt.xlabel('Confidence Threshold', fontsize=12)\n",
    "        plt.ylabel('Number of Detections', fontsize=12)\n",
    "        plt.title('Effect of Confidence Threshold on Detection Count', fontsize=14)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save\n",
    "        save_path = OUTPUTS_DIR / 'metrics' / 'confidence_threshold_analysis.png'\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nðŸ’¾ Saved: {save_path}\")\n",
    "        print(\"\\nðŸ’¡ Lower threshold = more detections (but may include false positives)\")\n",
    "        print(\"   Higher threshold = fewer detections (but higher confidence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Comparison Screenshot\n",
    "\n",
    "Show before/after detection on multiple frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(video_files) > 0:\n",
    "    # Select first video\n",
    "    video_path = video_files[0]\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Select 3 frames evenly spaced\n",
    "    frame_indices = [total_frames // 4, total_frames // 2, 3 * total_frames // 4]\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
    "    \n",
    "    for row, frame_idx in enumerate(frame_indices):\n",
    "        # Seek to frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if ret:\n",
    "            # Original frame\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            axes[row, 0].imshow(frame_rgb)\n",
    "            axes[row, 0].set_title(f\"Original Frame {frame_idx}\", fontsize=10)\n",
    "            axes[row, 0].axis('off')\n",
    "            \n",
    "            # Detected frame\n",
    "            results = detect_players(frame, model, conf_threshold=0.3)\n",
    "            annotated = results.plot()\n",
    "            annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            num_players = len(results.boxes)\n",
    "            axes[row, 1].imshow(annotated_rgb)\n",
    "            axes[row, 1].set_title(f\"Detected ({num_players} players)\", fontsize=10)\n",
    "            axes[row, 1].axis('off')\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    plt.suptitle(f\"Player Detection Results - {video_path.name}\", fontsize=14, y=0.995)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    save_path = OUTPUTS_DIR / 'screenshots' / 'detection_comparison.png'\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"ðŸ’¾ Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ“ Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "1. **YOLOv8 Architecture**: Single-stage detector for real-time object detection\n",
    "2. **Bounding Boxes**: Rectangular regions around detected objects\n",
    "3. **Confidence Scores**: Model's certainty about each detection\n",
    "4. **Class Filtering**: Detect only specific classes (e.g., persons)\n",
    "\n",
    "### Model Performance:\n",
    "- **Speed**: ~30-60 FPS on GPU, ~5-10 FPS on CPU\n",
    "- **Accuracy**: Pre-trained on COCO dataset (high quality)\n",
    "- **Robustness**: Works on various sports and lighting conditions\n",
    "\n",
    "### Common Issues:\n",
    "1. **False Positives**: Lower confidence threshold â†’ more false detections\n",
    "2. **Missed Detections**: Higher threshold â†’ miss some players\n",
    "3. **Occlusion**: Players overlapping may be detected as one\n",
    "4. **Small Objects**: Distant players harder to detect\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Checklist\n",
    "- [ ] YOLOv8 model loaded successfully\n",
    "- [ ] Detected players in test frames\n",
    "- [ ] Processed all videos with annotations\n",
    "- [ ] Generated detection statistics\n",
    "- [ ] Saved output videos and screenshots\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: Notebook 03 - Keypoint Detection (Pose Estimation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
