{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Player Tracking with ByteTrack\n",
    "\n",
    "**SC549: Neural Networks - Programming Assignment 03**\n",
    "\n",
    "In this notebook, we'll:\n",
    "1. Understand multi-object tracking (MOT)\n",
    "2. Implement ByteTrack algorithm\n",
    "3. Assign unique IDs to players\n",
    "4. Handle occlusions and re-identification\n",
    "5. Generate tracked videos\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What is Object Tracking?\n",
    "\n",
    "**Object Tracking** maintains identity of objects across video frames:\n",
    "- **Input**: Detections from each frame\n",
    "- **Output**: Same object gets same ID across all frames\n",
    "- **Challenge**: Handle occlusions, camera motion, similar appearances\n",
    "\n",
    "### Why Tracking is Hard:\n",
    "1. **Occlusion**: Players overlap ‚Üí disappear temporarily\n",
    "2. **Similar Appearance**: Players look alike\n",
    "3. **Fast Motion**: Large displacement between frames\n",
    "4. **ID Switches**: Tracker might confuse players\n",
    "\n",
    "### How ByteTrack Works:\n",
    "1. **Get detections** from YOLO for current frame\n",
    "2. **Predict** where previous tracks should be now\n",
    "3. **Match** new detections to predicted positions\n",
    "4. **Assign IDs** based on matches\n",
    "5. **Handle unmatched** detections (new players or lost tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "# YOLOv8 with tracking\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìπ Found 6 video(s)\n"
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "PROJECT_ROOT = Path('../')\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "VIDEOS_DIR = DATA_DIR / 'videos'\n",
    "OUTPUTS_DIR = PROJECT_ROOT / 'outputs'\n",
    "\n",
    "# Create directories\n",
    "(OUTPUTS_DIR / 'videos').mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUTS_DIR / 'screenshots').mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUTS_DIR / 'metrics').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Find videos\n",
    "video_files = list(VIDEOS_DIR.glob('*.mp4')) + list(VIDEOS_DIR.glob('*.avi'))\n",
    "print(f\"üìπ Found {len(video_files)} video(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Models\n",
    "\n",
    "We'll use:\n",
    "- **YOLOv8** for detection\n",
    "- **ByteTrack** (built into Ultralytics) for tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading models...\n",
      "‚úÖ Models loaded!\n",
      "   - Detection: YOLOv8s\n",
      "   - Pose: YOLOv8s-Pose\n",
      "   - Tracker: ByteTrack (built-in)\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "print(\"üì• Loading models...\")\n",
    "\n",
    "# Detection model\n",
    "detection_model = YOLO('yolov8s.pt')\n",
    "detection_model.to(device)\n",
    "\n",
    "# Pose model (for combined tracking)\n",
    "pose_model = YOLO('yolov8s-pose.pt')\n",
    "pose_model.to(device)\n",
    "\n",
    "print(\"‚úÖ Models loaded!\")\n",
    "print(\"   - Detection: YOLOv8s\")\n",
    "print(\"   - Pose: YOLOv8s-Pose\")\n",
    "print(\"   - Tracker: ByteTrack (built-in)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Tracking on Single Video\n",
    "\n",
    "Let's see how tracking works on one video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking: input_video_1.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tracking:   0%|‚ñè                                                             | 1/297 [00:02<12:03,  2.44s/it]"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.13.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\drawing.cpp:1835: error: (-215:Assertion failed) 0 < thickness && thickness <= MAX_THICKNESS in function 'cv::line'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 113\u001b[39m\n\u001b[32m    110\u001b[39m     test_video = video_files[\u001b[32m0\u001b[39m]\n\u001b[32m    111\u001b[39m     output_path = OUTPUTS_DIR / \u001b[33m'\u001b[39m\u001b[33mvideos\u001b[39m\u001b[33m'\u001b[39m / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtracked_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_video.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     stats, history = \u001b[43mtrack_players_simple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetection_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müíæ Saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mtrack_players_simple\u001b[39m\u001b[34m(video_path, model, output_path, show_trajectory)\u001b[39m\n\u001b[32m     75\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(points)):\n\u001b[32m     76\u001b[39m                 thickness = \u001b[38;5;28mint\u001b[39m(np.sqrt(i / \u001b[32m30\u001b[39m) * \u001b[32m3\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m                 \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthickness\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     79\u001b[39m     annotated = frame.copy()\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.13.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\drawing.cpp:1835: error: (-215:Assertion failed) 0 < thickness && thickness <= MAX_THICKNESS in function 'cv::line'\n"
     ]
    }
   ],
   "source": [
    "def track_players_simple(video_path, model, output_path=None, show_trajectory=True):\n",
    "    \"\"\"\n",
    "    Track players in a video with ByteTrack.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video\n",
    "        model: YOLO model\n",
    "        output_path: Path to save output (optional)\n",
    "        show_trajectory: Draw trajectory trails\n",
    "    \n",
    "    Returns:\n",
    "        dict: Tracking statistics\n",
    "    \"\"\"\n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Video writer\n",
    "    out = None\n",
    "    if output_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"Tracking: {video_path.name}\")\n",
    "    \n",
    "    # Track history for trajectory\n",
    "    track_history = defaultdict(lambda: [])\n",
    "    \n",
    "    # Statistics\n",
    "    unique_ids = set()\n",
    "    frame_count = 0\n",
    "    total_detections = 0\n",
    "    \n",
    "    pbar = tqdm(total=total_frames, desc=\"Tracking\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Run tracking\n",
    "        # persist=True enables ByteTrack\n",
    "        # classes=[0] = only track persons\n",
    "        results = model.track(frame, persist=True, classes=[0], conf=0.3, verbose=False)\n",
    "        \n",
    "        # Get tracking results\n",
    "        if results[0].boxes.id is not None:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "            track_ids = results[0].boxes.id.cpu().numpy().astype(int)\n",
    "            \n",
    "            total_detections += len(track_ids)\n",
    "            unique_ids.update(track_ids)\n",
    "            \n",
    "            # Draw trajectory\n",
    "            annotated = results[0].plot()\n",
    "            \n",
    "            if show_trajectory:\n",
    "                for box, track_id in zip(boxes, track_ids):\n",
    "                    x1, y1, x2, y2 = box\n",
    "                    cx = int((x1 + x2) / 2)\n",
    "                    cy = int((y1 + y2) / 2)\n",
    "                    \n",
    "                    # Add to history\n",
    "                    track_history[track_id].append((cx, cy))\n",
    "                    \n",
    "                    # Keep only last 30 points\n",
    "                    if len(track_history[track_id]) > 30:\n",
    "                        track_history[track_id].pop(0)\n",
    "                    \n",
    "                    # Draw trajectory\n",
    "                    points = track_history[track_id]\n",
    "                    for i in range(1, len(points)):\n",
    "                        # FIX: Ensure thickness is always at least 1\n",
    "                        thickness = max(1, int(np.sqrt(i / 30) * 3))\n",
    "                        cv2.line(annotated, points[i-1], points[i], (0, 255, 255), thickness)\n",
    "        else:\n",
    "            annotated = frame.copy()\n",
    "        \n",
    "        # Write frame\n",
    "        if out:\n",
    "            out.write(annotated)\n",
    "        \n",
    "        frame_count += 1\n",
    "        pbar.update(1)\n",
    "    \n",
    "    cap.release()\n",
    "    if out:\n",
    "        out.release()\n",
    "    pbar.close()\n",
    "    \n",
    "    # Stats\n",
    "    stats = {\n",
    "        'total_frames': frame_count,\n",
    "        'unique_ids': len(unique_ids),\n",
    "        'total_detections': total_detections,\n",
    "        'avg_detections_per_frame': total_detections / frame_count if frame_count > 0 else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"‚úÖ Tracking complete!\")\n",
    "    print(f\"   Unique player IDs: {stats['unique_ids']}\")\n",
    "    print(f\"   Total detections: {stats['total_detections']}\")\n",
    "    print(f\"   Avg per frame: {stats['avg_detections_per_frame']:.2f}\\n\")\n",
    "    \n",
    "    return stats, track_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Understanding Tracking Results\n",
    "\n",
    "Let's analyze what tracking gives us beyond detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(video_files) > 0:\n",
    "    print(\"üìä Tracking vs Detection:\\n\")\n",
    "    print(\"Detection Only:\")\n",
    "    print(\"  - Frame 1: [Person, Person, Person]\")\n",
    "    print(\"  - Frame 2: [Person, Person, Person]\")\n",
    "    print(\"  ‚Üí Who is who? No idea!\\n\")\n",
    "    \n",
    "    print(\"With Tracking:\")\n",
    "    print(\"  - Frame 1: [Player ID=1, Player ID=2, Player ID=3]\")\n",
    "    print(\"  - Frame 2: [Player ID=1, Player ID=2, Player ID=3]\")\n",
    "    print(\"  ‚Üí Same ID = same player!\\n\")\n",
    "    \n",
    "    print(\"üìà Benefits:\")\n",
    "    print(\"  1. Count unique players\")\n",
    "    print(\"  2. Analyze individual player movement\")\n",
    "    print(\"  3. Create player trajectories\")\n",
    "    print(\"  4. Measure player-specific statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Player Trajectories\n",
    "\n",
    "Show movement paths of each player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(video_files) > 0 and history:\n",
    "    # Plot trajectories\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Get video frame for background\n",
    "    cap = cv2.VideoCapture(str(test_video))\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if ret:\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(frame_rgb, alpha=0.5)\n",
    "    \n",
    "    # Plot each player's trajectory\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(history)))\n",
    "    \n",
    "    for (track_id, points), color in zip(history.items(), colors):\n",
    "        if len(points) > 1:\n",
    "            xs = [p[0] for p in points]\n",
    "            ys = [p[1] for p in points]\n",
    "            \n",
    "            # Plot trajectory\n",
    "            ax.plot(xs, ys, '-o', color=color, linewidth=2, \n",
    "                   markersize=4, label=f'Player {track_id}', alpha=0.7)\n",
    "            \n",
    "            # Mark start and end\n",
    "            ax.plot(xs[0], ys[0], 'go', markersize=10, label='_nolegend_')  # Start: green\n",
    "            ax.plot(xs[-1], ys[-1], 'ro', markersize=10, label='_nolegend_')  # End: red\n",
    "    \n",
    "    ax.set_title(f\"Player Trajectories - {test_video.name}\", fontsize=14)\n",
    "    ax.legend(loc='best')\n",
    "    ax.set_xlabel('X position (pixels)')\n",
    "    ax.set_ylabel('Y position (pixels)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save\n",
    "    save_path = OUTPUTS_DIR / 'screenshots' / 'player_trajectories.png'\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"üíæ Saved: {save_path}\")\n",
    "    print(\"\\nüü¢ Green dot = Start position\")\n",
    "    print(\"üî¥ Red dot = End position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Combined: Detection + Pose + Tracking\n",
    "\n",
    "Now let's combine everything: detect players, estimate poses, and track with IDs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_with_pose(video_path, pose_model, output_path):\n",
    "    \"\"\"\n",
    "    Full pipeline: detection + pose estimation + tracking.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video\n",
    "        pose_model: YOLOv8-Pose model\n",
    "        output_path: Path to save output\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"Processing: {video_path.name}\")\n",
    "    print(\"  With: Detection + Pose + Tracking\")\n",
    "    \n",
    "    pbar = tqdm(total=total_frames, desc=\"Processing\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Track with pose estimation\n",
    "        results = pose_model.track(frame, persist=True, conf=0.3, verbose=False)\n",
    "        \n",
    "        # Annotate (includes boxes, IDs, and skeletons)\n",
    "        annotated = results[0].plot()\n",
    "        \n",
    "        out.write(annotated)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    pbar.close()\n",
    "    \n",
    "    print(f\"‚úÖ Complete! Saved: {output_path}\\n\")\n",
    "\n",
    "# Process all videos with full pipeline\n",
    "if len(video_files) > 0:\n",
    "    print(\"üé¨ Processing all videos with FULL PIPELINE...\\n\")\n",
    "    \n",
    "    for video_path in video_files:\n",
    "        output_path = OUTPUTS_DIR / 'videos' / f\"final_{video_path.name}\"\n",
    "        track_with_pose(video_path, pose_model, output_path)\n",
    "    \n",
    "    print(\"\\n‚úÖ All videos processed with full pipeline!\")\n",
    "    print(f\"üìÅ Final outputs: {OUTPUTS_DIR / 'videos'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No videos found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Side-by-Side Comparison\n",
    "\n",
    "Show: Original ‚Üí Detection ‚Üí Pose ‚Üí Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(video_files) > 0:\n",
    "    video_path = video_files[0]\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    \n",
    "    # Get a middle frame\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, total_frames // 2)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if ret:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # 1. Original\n",
    "        axes[0, 0].imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        axes[0, 0].set_title(\"1. Original Frame\", fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].axis('off')\n",
    "        \n",
    "        # 2. Detection only\n",
    "        det_results = detection_model(frame, classes=[0], conf=0.3, verbose=False)\n",
    "        det_annotated = det_results[0].plot()\n",
    "        axes[0, 1].imshow(cv2.cvtColor(det_annotated, cv2.COLOR_BGR2RGB))\n",
    "        axes[0, 1].set_title(\"2. Player Detection\", fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].axis('off')\n",
    "        \n",
    "        # 3. Pose estimation\n",
    "        pose_results = pose_model(frame, conf=0.3, verbose=False)\n",
    "        pose_annotated = pose_results[0].plot()\n",
    "        axes[1, 0].imshow(cv2.cvtColor(pose_annotated, cv2.COLOR_BGR2RGB))\n",
    "        axes[1, 0].set_title(\"3. Pose Estimation\", fontsize=12, fontweight='bold')\n",
    "        axes[1, 0].axis('off')\n",
    "        \n",
    "        # 4. Full pipeline with tracking\n",
    "        track_results = pose_model.track(frame, persist=True, conf=0.3, verbose=False)\n",
    "        track_annotated = track_results[0].plot()\n",
    "        axes[1, 1].imshow(cv2.cvtColor(track_annotated, cv2.COLOR_BGR2RGB))\n",
    "        axes[1, 1].set_title(\"4. Detection + Pose + Tracking\", fontsize=12, fontweight='bold')\n",
    "        axes[1, 1].axis('off')\n",
    "        \n",
    "        plt.suptitle(\"Complete Player Tracking Pipeline\", fontsize=16, fontweight='bold', y=0.98)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save\n",
    "        save_path = OUTPUTS_DIR / 'screenshots' / 'pipeline_comparison.png'\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"üíæ Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Tracking Quality Metrics\n",
    "\n",
    "Analyze tracking consistency and ID switches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(video_files) > 0:\n",
    "    print(\"üìä Tracking Quality Metrics:\\n\")\n",
    "    \n",
    "    print(\"Key Metrics to Evaluate:\")\n",
    "    print(\"1. **ID Consistency**: Same player keeps same ID\")\n",
    "    print(\"2. **ID Switches**: When tracker confuses players\")\n",
    "    print(\"3. **Fragmentation**: Player disappears and reappears with new ID\")\n",
    "    print(\"4. **MOTA** (Multiple Object Tracking Accuracy)\")\n",
    "    print(\"5. **MOTP** (Multiple Object Tracking Precision)\\n\")\n",
    "    \n",
    "    print(\"üí° For this assignment:\")\n",
    "    print(\"   - Count unique IDs per video\")\n",
    "    print(\"   - Check if IDs remain consistent visually\")\n",
    "    print(\"   - Note any obvious ID switches in report\")\n",
    "    print(\"\\n   (Ground truth not available for full MOTA/MOTP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "### What We Learned:\n",
    "1. **Multi-Object Tracking**: Assigning persistent IDs to objects\n",
    "2. **ByteTrack Algorithm**: Motion-based association\n",
    "3. **Trajectory Visualization**: Understanding player movement\n",
    "4. **Combined Pipeline**: Detection + Pose + Tracking\n",
    "\n",
    "### Tracking Challenges:\n",
    "1. **Occlusion**: Players overlap ‚Üí temporary ID loss\n",
    "2. **Similar Appearance**: Uniform colors make tracking harder\n",
    "3. **Fast Motion**: Large movements between frames\n",
    "4. **Camera Motion**: Background movement confuses tracker\n",
    "\n",
    "### Applications:\n",
    "1. **Player Statistics**: Count touches, distance run, speed\n",
    "2. **Team Formation**: Analyze positioning over time\n",
    "3. **Event Detection**: Passes, tackles, shots\n",
    "4. **Heatmaps**: Where players spend most time\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Checklist\n",
    "- [ ] ByteTrack tracking implemented\n",
    "- [ ] Unique IDs assigned to players\n",
    "- [ ] Trajectories visualized\n",
    "- [ ] Full pipeline working (detection + pose + tracking)\n",
    "- [ ] Output videos generated with IDs\n",
    "\n",
    "---\n",
    "\n",
    "**Next**: Notebook 05 - Performance Evaluation & Metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
